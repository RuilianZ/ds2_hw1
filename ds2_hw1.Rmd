---
title: "Homework 1 for Data Science II"
author: "Roxy Zhang (rz2570)"
date: "2/12/2022"
output: pdf_document
---

### Setup and import data

```{r, message = FALSE}
library(tidyverse)
library(caret)
library(corrplot)
library(leaps)
```

```{r, message = FALSE}
train_df = 
  read_csv("data/housing_training.csv") %>% 
  janitor::clean_names() %>% 
  na.omit()

test_df = 
  read_csv("data/housing_test.csv") %>% 
  janitor::clean_names() %>% 
  na.omit()

dim(train_df)
table(sapply(train_df[ , -1], class)) %>% 
  knitr::kable()

dim(test_df)
table(sapply(test_df[ , -1], class)) %>% 
  knitr::kable()
```

* We want to predict outcome variable `sale_price` by selecting predictors from `r ncol(train_df)` variables, among which there are 4 categorical variables and 21 continuous variables.   
* There are `r nrow(train_df)` data in training data and `r nrow(test_df)` in test data.  


### Data preparation

For the convenience of fitting models, we want to create vectors and matrices in advance:

```{r}
train_x <- model.matrix(sale_price ~ ., train_df)[ ,-1]
train_y <- train_df$sale_price
test_x <- model.matrix(sale_price ~ ., test_df)[ , -1]
test_y <- test_df$sale_price
test_all <- model.matrix(sale_price ~ ., test_df)
```

In linear regression, correlation among variables can cause large variance and make interpretation harder.  
So we want to have a look and the potential correlation among predictors:

```{r}
corrplot(
  cor(train_x), 
  method = "circle",
  type = "full",
  tl.cex = .5, 
  tl.col = "darkblue")
```

* From the plot above, we can see some positive correlations in the upright corner among some area-related predictors such as `gr_liv_area` and `second_flr_sf`, and also negative correlations among categorical variables such as `exter_qualTypical` and `kitchen_qualTypical`.  
* To reduce the influence of correlation, we may want to reduce the number of predictors using best subset model selection.  

```{r}
lm_subsets <- regsubsets(sale_price ~ .,
                         data = train_df,
                         method = "exhaustive",
                         nbest = 6)

plot(lm_subsets, scale = "bic")

# Look at actual numbers of predictors
# summary(lm_subsets)
```
 
* The plot above gives us a sense of model selsction. Hoever, we will stick to using all the predictors in this assignment.


## Least Squares

Fit a linear model using least squares on the training data. Is there any potential disadvantage of this model?

### Model fitting

```{r, warning = FALSE}
set.seed(2570)

# Specify resampling method
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Fit a linear model using caret
lm_fit <- train(sale_price ~ .,
                data = train_df,
                method = "lm",
                preProcess = c("center", "scale"),
                trControl = ctrl)
 
# Extract coefficiencts
round(lm_fit$finalModel$coefficients, 3) %>% 
  knitr::kable()

# Calculate mean training RMSE
mean(lm_fit$resample$RMSE)
```

### Make prediction

```{r}
# Make prediction on test data
lm_predict <- predict(lm_fit, newdata = test_df)

# Calculate test RMSE
RMSE(lm_predict, test_df$sale_price)
```

**Potential disadvantage** 

1. The model contains too many predictors, which is hard for interpretation.    
2. As seen above, there are correlations among predictors, which may lead to: 1. higher variance and RMSE 2. less prediction accuracy 3. difficulty for interpretation  
3. Due to the nature of its modeling method, Least Squares is sensitive to outliers.  
4. Large data set is necessary in order to obtain reliable results. Our sample in this case might not be large enough.  


## Lasso

Fit a lasso model on the training data and report the test error. When the 1SE rule is applied, how many predictors are included in the model?

### Model fitting

```{r}
set.seed(2570)

# Specify resampling method for 1SE
ctrl_1se <- trainControl(method = "repeatedcv", number = 10, repeats = 5, selectionFunction = "oneSE")

# Fit a lasso model using caret
# The fitted model is a glemnet object, so we need to use matrix as input
lasso_fit <- train(x = train_x, 
                   y = train_y,
                   method = "glmnet",
                   preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(5, -1, length = 100))),
                   trControl = ctrl_1se)

# Plot RMSE against lambda
plot(lasso_fit, xTrans = log)

# Extract optimum lambda
lasso_fit$bestTune

# Extract coefficiencts
as.matrix(round(coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda), 3)) %>% 
  knitr::kable()
```

* From the fitted lasso model, we can see that the optimum lambda chosen is `r lasso_fit$bestTune$lambda`

### Make prediction

```{r}
set.seed(2570)

# Make prediction on test data
lasso_predict <- predict(lasso_fit, newdata = test_all)

# Calculate test RMSE
RMSE(lasso_predict, test_df$sale_price)
```

**Test error and number of predictors**

* The test RMSE is `r RMSE(lasso_predict, test_df$sale_price)`.  
* When the 1SE rule is applied, there are `r data.frame(as.matrix(coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda))) %>% filter(s1 != 0) %>% nrow()` predictors included in the model.  


## Elastic Net

Fit an elastic net model on the training data. Report the selected tuning parameters and the test error.

### Model fitting

```{r}
set.seed(2570)

# Fit a elastic net model
enet_fit <- train(x = train_x,
                  y = train_y,
                  method = "glmnet",
                  preProcess = c("center", "scale"),
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 23),
                                         lambda = exp(seq(-2, 6, length = 50))),
                  trControl = ctrl)

# Plot RMSE against lambda
plot(enet_fit, xTrans = log, xlim = c(-2, 6))

# Extract optimum lambda
enet_fit$bestTune

# Extract coefficiencts
as.matrix(round(coef(enet_fit$finalModel, enet_fit$bestTune$lambda), 3)) %>% 
  knitr::kable()
```

* From the fitted elastic net model, we can see that the optimum alpha chosen is `r enet_fit$bestTune$alpha`, and the optimum lambda chosen is `r enet_fit$bestTune$lambda`.  
* Since the optimum alpha is much closer to 0 than 1, we can say that the optimum elastic net model behaves closer to ridge than lasso.

### Make prediction

```{r}
set.seed(2570)

# Make prediction on test data
enet_predict <- predict(enet_fit, newdata = test_all)

# Calculate test RMSE
RMSE(enet_predict, test_df$sale_price)
```

**Selected tuning parameters and test error**

* The selected tuning parameter lambda is `r enet_fit$bestTune$lambda`.  
* The test RMSE is `r RMSE(enet_predict, test_df$sale_price)`.


## Partial Least Squares

Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

### Model fitting

```{r}
set.seed(2570)

# Fit a partial least squares model
pls_fit <- train(x = train_x,
                  y = train_y,
                  method = "pls",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(ncomp = 1:23),
                  trControl = ctrl)

# Plot RMSE against number of components
ggplot(pls_fit, highlight = TRUE) +
  theme_bw()

# Extract best tuning parameter
pls_fit$bestTune
```

* From the fitted partial least squares model, we can see that the number of components is `r pls_fit$bestTune$ncomp`.  
* The highlighted dot in the plot also shows the same result.

### Make prediction

```{r}
set.seed(2570)

# Make prediction on test data
pls_predict <- predict(pls_fit, newdata = test_all)

# Calculate test RMSE
RMSE(pls_predict, test_df$sale_price)
```

**Number of components and test error**

* The number of components is `r pls_fit$bestTune$ncomp`, 
* The test RMSE is `r RMSE(pls_predict, test_df$sale_price)`.


## Model selection

Which model will you choose for predicting the response? Why?

```{r}
# Compare the models based on resampling results
resamp<- resamples(list(ls = lm_fit,
                        lasso = lasso_fit,
                        enet = enet_fit,
                        pls = pls_fit))

summary(resamp)

bwplot(resamp, metric = "RMSE")

parallelplot(resamp, metric = "RMSE")
```

* Based on the plots above, we would likely to choose the **least squares** model, since it has the lowest RMSE compared to other models (however, the difference of average RMSE and the range of RMSE among groups is relatively small). We have to admit that sometimes "Simplicity is the ultimate sophistication" :)
